---
# Documentation: https://arxiv.org/pdf/2506.05205

title: "RELIC: Evaluating Compositional Instruction Following via Language Recognition"
authors: ["Jackson Petty", "Michael Y. Hu", "Wentao Wang", "Shauli Ravfogel", "William Merrill", "Tal Linzen"]

date: 2025-06-01
doi: ""

# Schedule page publish date (NOT publication's date).
publishDate: 2025-06-01T15:16:19+02:00

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["3"]

# Publication name and optional abbreviated publication name.
publication: "RELIC: Evaluating Compositional Instruction Following via Language Recognition"
publication_short: ""

abstract: "Large language models (LLMs) are increasingly expected to perform tasks based only on a specification of the task provided in context, without examples of inputs and outputs; this ability is referred to as instruction following. We introduce the Recognition of Languages In-Context (RELIC) framework to evaluate instruction following using language recognition: the task of determining if a string is generated by formal grammar. Unlike many standard evaluations of LLMs' ability to use their context, this task requires composing together a large number of instructions (grammar productions) retrieved from the context. Because the languages are synthetic, the task can be increased in complexity as LLMs' skills improve, and new instances can be automatically generated, mitigating data contamination. We evaluate state-of-the-art LLMs on RELIC and find that their accuracy can be reliably predicted from the complexity of the grammar and the individual example strings, and that even the most advanced LLMs currently available show near-chance performance on more complex grammars and samples, in line with theoretical expectations. We also use RELIC to diagnose how LLMs attempt to solve increasingly difficult reasoning tasks, finding that as the complexity of the language recognition task increases, models switch to relying on shallow heuristics instead of following complex instructions. "

# Summary. An optional shortened abstract.
summary: "We present RELIC, a benchmark that lets us probe instruction following through language-recognition tasks. In RELIC we ask an LLM to decide whether a string belongs to a formal grammar, forcing it to compose many grammar rules retrieved from context. Because the grammars are synthetic, we can tune their complexity up or down and generate unlimited new samples—so there’s no data leakage. Testing today’s best LLMs, we find accuracy drops to chance once the grammars get moderately complex; and, concurrently, model behaviour shifts from true rule-following to shallow heuristics."

tags: []
categories: []
featured: false

# Custom links (optional).
#   Uncomment and edit lines below to show custom links.
# links:
# - name: Follow
#   url: https://twitter.com
#   icon_pack: fab
#   icon: twitter

url_pdf: "https://arxiv.org/pdf/2506.05205"
url_code: ""
url_dataset:
url_poster: ""
url_project:
url_slides: ""
url_source:
url_video: 

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: "Center"
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---

